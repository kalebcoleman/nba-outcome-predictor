devtools::load_all()
devtools::document()
devtools::load_all()
train_feats <- build_features_all(2002:2024)
head(train_feats)
View(train_feats)
devtools::load_all()
train_feats <- build_features_all(2002:2024)
devtools::load_all()
train_feats <- build_features_all(2002:2024)
head(train_feats)
View(train_feats)
devtools::document()
devtools::load_all()
prepare_train_test()
View(test_feats)
View(train_feats)
View(train_feats)
View(test_feats)
devtools::load_all(".")
prepare_train_test()
train_feats <- prepare_train_test()$train_feats
test_feats  <- prepare_train_test()$test_feats
View(test_feats)
# 1) Check for any NAs in each column
sapply(train_feats, function(x) sum(is.na(x)))
# 2) Quick summary of all numeric predictors
numeric_cols <- sapply(train_feats, is.numeric)
summary(train_feats[, numeric_cols])
library(dplyr)
# Drop any rows with missing stats
train_feats_clean <- train_feats %>%
tidyr::drop_na()
# How many did we drop?
n_dropped <- nrow(train_feats) - nrow(train_feats_clean)
message("Dropped ", n_dropped, " games with incomplete stats.")
# Verify no more NAs
sapply(train_feats_clean, function(x) sum(is.na(x)))
train_feats <- prepare_train_test()$train_feats
test_feats  <- prepare_train_test()$test_feats
devtools::load_all(".")
train_feats <- prepare_train_test()$train_feats
test_feats  <- prepare_train_test()$test_feats
View(train_feats)
View(test_feats)
devtools::load_all(".")
train_feats <- prepare_train_test()$train_feats
test_feats  <- prepare_train_test()$test_feats
View(test_feats)
View(train_feats)
View(test_feats)
devtools::load_all(".")
train_feats <- prepare_train_test()$train_feats
test_feats  <- prepare_train_test()$test_feats
View(test_feats)
View(train_feats)
train_feats <- prepare_train_test()$train_feats
train_feats <- prepare_train_test()$train_feats
test_feats  <- prepare_train_test()$test_feats
View(test_feats)
View(train_feats)
devtools::load_all(".")
# 1) Fit a simple logistic‐regression on Elo difference
elo_model <- glm(
Target ~ Elo_Diff,
data   = train_feats,
family = binomial(link = "logit")
)
# 2) Check the model summary
summary(elo_model)
# 3) Predict probabilities on test data
test_probs <- predict(elo_model, newdata = test_feats, type = "response")
# 4) Convert to binary predictions (0/1) at 0.5 threshold
test_preds <- ifelse(test_probs > 0.5, 1, 0)
# 5) Compute accuracy
mean(test_preds == test_feats$Target)
library(pROC)
roc_obj <- roc(test_feats$Target, test_probs)
auc(roc_obj)
table(
Predicted = test_preds,
Actual    = test_feats$Target
)
# Compute sensitivity / specificity
library(caret)
install.packages("cart")
install.packages("caret")
table(
Predicted = test_preds,
Actual    = test_feats$Target
)
# Compute sensitivity / specificity
library(caret)
conf <- confusionMatrix(
factor(test_preds, levels=c(0,1)),
factor(test_feats$Target, levels=c(0,1))
)
conf$byClass[c("Sensitivity","Specificity")]
coords <- coords(roc_obj, x = "best", best.method = "youden", ret = c("threshold","sensitivity","specificity"))
coords
opt_thresh <- coords["threshold"]
test_preds_opt <- ifelse(test_probs > opt_thresh, 1, 0)
conf_opt <- confusionMatrix(
factor(test_preds_opt, levels=c(0,1)),
factor(test_feats$Target, levels=c(0,1))
)
# 1) Pull threshold as a numeric
opt_thresh <- as.numeric(coords[1, "threshold"])
# 2) Recompute predictions
test_preds_opt <- ifelse(test_probs > opt_thresh, 1, 0)
# 3) Confusion at that threshold
conf_opt <- confusionMatrix(
factor(test_preds_opt, levels = c(0,1)),
factor(test_feats$Target, levels = c(0,1))
)
# 4) Review balanced sensitivity/specificity and accuracy
conf_opt$byClass[c("Sensitivity","Specificity")]
mean(test_preds_opt == test_feats$Target)
devtools::load_all(".")
# source or load your package
devtools::load_all()
# fit your baseline Elo‐only model
elo_model <- glm(Target ~ Elo_Diff, data = train_feats, family = binomial)
# 1) Get metrics at 0.5 threshold
res0.5 <- summarize_model(elo_model, test_feats, thresh = 0.5)
print(res0.5$accuracy); print(res0.5$sensitivity); print(res0.5$specificity)
print(res0.5$confusion)
# 2) Find optimal threshold
roc_obj  <- pROC::roc(test_feats$Target, predict(elo_model, newdata = test_feats, type="response"))
opt <- pROC::coords(roc_obj, "best", best.method="youden", ret="threshold")
res_opt <- summarize_model(elo_model, test_feats, thresh = opt)
# 2a) Pull out the threshold as a plain scalar
opt_thresh <- as.numeric(opt[1])
# 2b) Summarize at that threshold
res_opt <- summarize_model(elo_model, test_feats, thresh = opt_thresh)
# 2c) Inspect
print(res_opt$accuracy)
print(res_opt$sensitivity)
print(res_opt$specificity)
print(res_opt$confusion)
# source or load your package
devtools::load_all()
# fit your baseline Elo‐only model
elo_model <- glm(Target ~ Elo_Diff, data = train_feats, family = binomial)
# 1) Get metrics at 0.5 threshold
res0.5 <- summarize_model(elo_model, test_feats, thresh = 0.5)
print(res0.5$accuracy); print(res0.5$sensitivity); print(res0.5$specificity)
print(res0.5$confusion)
# 2) Find optimal threshold
roc_obj  <- pROC::roc(test_feats$Target, predict(elo_model, newdata = test_feats, type="response"))
opt <- pROC::coords(roc_obj, "best", best.method="youden", ret="threshold")
res_opt <- summarize_model(elo_model, test_feats, thresh = opt)
devtools::load_all(".")
res <- prepare_train_test()
train_feats <- res$train_feats
test_feats  <- res$test_feats
View(test_feats)
elo_model <- glm(Target ~ Elo_Diff + AllTimeElo_Diff,
data   = train_feats,
family = binomial)
library(pROC)
probs     <- predict(elo_model, newdata = test_feats, type = "response")
roc_obj   <- roc(test_feats$Target, probs)
auc_val   <- auc(roc_obj)
opt_thresh <- coords(roc_obj, "best", best.method="youden", ret="threshold") %>% as.numeric()
library(caret)
preds_0.5 <- ifelse(probs > 0.5, 1, 0)
preds_opt <- ifelse(probs > opt_thresh, 1, 0)
mean(preds_0.5 == test_feats$Target)  # accuracy at 0.5
mean(preds_opt == test_feats$Target)  # accuracy at optimal threshold
print(opt_thresh)
coords(roc_obj,
x           = "best",
best.method = "youden",
ret         = c("threshold","sensitivity","specificity"))
final_model <- glm(Target ~ Elo_Diff + AllTimeElo_Diff,
data = train_feats, family = binomial)
present_model(final_model, train_feats, test_feats)
devtools::load_all(".")
present_model(final_model, train_feats, test_feats)
present_model(final_model, train_feats, test_feats)
usethis::use_data(final_model, overwrite = TRUE)
devtools::load_all(".")
model <- load_model()
train_feats <- res$train_feats
train_feats <- prepare_train_test()$train_feats
devtools::load_all(".")
train_feats <- prepare_train_test()$train_feats
test_feats  <- prepare_train_test()$test_feats
test_feats  <- prepare_train_test()$test_feats
present_model(model, train_feats, test_feats)
devtools::load_all(".")
present_model(model, train_feats, test_feats)
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all()
res <- prepare_train_test()
train_feats <- res$train_feats
test_feats  <- res$test_feats
# Check new columns
names(train_feats)
# … should include roll5_net_home, roll5_net_away, roll5_efg_home, roll5_efg_away
# Inspect a few rows
head(train_feats[c("roll5_net_home","roll5_net_away","roll5_efg_home","roll5_efg_away")])
View(test_feats)
library(dplyr)
# 1) How many NAs per rolling feature in train_feats?
sapply(train_feats[, c("roll5_net_home","roll5_net_away",
"roll5_efg_home","roll5_efg_away")],
function(x) sum(is.na(x)))
window <- 5
# gather into long form so we can test per-team per-side
tb_long <- train_feats %>%
select(game_id, game_date,
home = home_team, away = away_team,
roll5_net_home, roll5_net_away,
roll5_efg_home, roll5_efg_away) %>%
pivot_longer(
cols = starts_with("roll5_"),
names_to = c("stat","side"),
names_pattern = "roll5_(.*)_(home|away)",
values_to = "value"
) %>%
# attach the team name per row
mutate(
team = if_else(side == "home", home, away)
) %>%
arrange(team, game_date) %>%
group_by(team, stat) %>%
mutate(
rn = row_number(),
is_na = is.na(value),
correct_na = if_else(rn < window, TRUE, FALSE)
) %>%
ungroup()
# 2a) Any NAs beyond the first (window−1) rows?
leakage <- tb_long %>%
filter(rn >= window, is_na)
if (nrow(leakage) == 0) {
message("✅ No unexpected NAs after the first ", window-1, " games per team.")
} else {
warning("💥 Found NAs beyond the first ", window-1, " games! Investigate these rows:")
print(leakage)
}
# 2b) Confirm that the only NAs are in rn < window
bad_na <- tb_long %>%
filter(rn < window, !is_na)
if (nrow(bad_na) == 0) {
message("✅ All first ", window-1, " rows per team are NA as expected.")
} else {
warning("💥 Some of the first ", window-1, " games are NOT NA when they should be:")
print(bad_na)
}
devtools::load_all(".")
sapply(train_feats[, c("roll5_net_home","roll5_net_away",
"roll5_efg_home","roll5_efg_away")],
function(x) sum(is.na(x)))
library(dplyr)
# 1) Pick your team (exact match on home/away columns)
team_name <- "Los Angeles Lakers"
# 2) Filter for that team’s games, sort by date, take first 10
team_games <- train_feats %>%
filter(home_team == team_name | away_team == team_name) %>%
arrange(game_date) %>%
head(10) %>%
# 3) Compute side-specific roll_net & roll_efg
mutate(
side     = if_else(home_team == team_name, "home", "away"),
roll_net = if_else(side == "home", roll5_net_home, roll5_net_away),
roll_efg = if_else(side == "home", roll5_efg_home, roll5_efg_away)
) %>%
select(game_date, side, roll_net, roll_efg)
# 4) Inspect
print(team_games)
View(test_feats)
devtools::load_all(".")
train_feats <- prepare_train_test()$train_feats
test_feats  <- prepare_train_test()$test_feats
View(test_feats)
# Verify for Lakers
train_feats %>%
filter(home_team=="Los Angeles Lakers" | away_team=="Los Angeles Lakers") %>%
arrange(game_date) %>%
slice(1:5) %>%
transmute(game_date, side = if_else(home_team=="Los Angeles Lakers","home","away"),
roll5_net = if_else(side=="home",roll5_net_home,roll5_net_away),
roll5_efg = if_else(side=="home",roll5_efg_home,roll5_efg_away))
library(dplyr)
team_name <- "Los Angeles Lakers"
train_feats %>%
filter(home_team == team_name | away_team == team_name) %>%
arrange(game_date) %>%
slice(1:10) %>%
select(
game_date,
home_score,
away_score,
roll5_net_home,
roll5_net_away,
roll5_efg_home,
roll5_efg_away
) %>%
print(n = Inf)
library(dplyr)
team_name <- "Los Angeles Lakers"
train_feats %>%
filter(home_team == team_name | away_team == team_name) %>%
arrange(game_date) %>%
slice(1:10) %>%
select(
game_date,
home_score,
away_score,
roll5_net_home,
roll5_net_away,
roll5_efg_home,
roll5_efg_away
) %>%
print(n = Inf)
library(dplyr)
team_name <- "Los Angeles Lakers"
# 1) Pull out the first 6 games for Lakers from the raw box data
laker_games <- do.call(rbind, lapply(2001:2001, function(y) {
hoopR::load_nba_team_box(seasons = y) %>%
filter(season_type == 2,
team_display_name == team_name) %>%
transmute(
game_id,
game_date,
side     = team_home_away,
net      = team_score - opponent_team_score,
efg      = (field_goals_made + 0.5*three_point_field_goals_made) /
field_goals_attempted
)
})) %>%
arrange(game_date) %>%
slice(1:6)
library(dplyr)
library(tidyr)
team_name <- "Los Angeles Lakers"
window    <- 5
seasons   <- 2002  # start at 2002 since hoopR data begins there
# 1) Get raw box data for Lakers in 2002
raw_lakers <- hoopR::load_nba_team_box(seasons = seasons) %>%
filter(season_type == 2,
team_display_name == team_name) %>%
transmute(
game_id,
game_date,
side   = team_home_away,
net    = team_score - opponent_team_score,
efg    = (field_goals_made + 0.5*three_point_field_goals_made) /
field_goals_attempted
) %>%
arrange(game_date) %>%
slice(1:6)
# 2) Pull in your pre-computed rolling stats
recent <- compute_recent_stats(seasons = seasons, window = window)
# 3) Merge together
check_df <- raw_lakers %>%
left_join(recent, by = "game_id") %>%
# pick the correct rolling column per side
mutate(
roll_net = if_else(side == "home", roll5_net_home, roll5_net_away),
roll_efg = if_else(side == "home", roll5_efg_home, roll5_efg_away)
) %>%
select(game_date, side, net, efg, roll_net, roll_efg)
print(check_df)
library(dplyr)
library(tidyr)
team_name <- "Los Angeles Lakers"
window    <- 5
seasons   <- 2002  # start at 2002 since hoopR data begins there
# 1) Get raw box data for Lakers in 2002
raw_lakers <- hoopR::load_nba_team_box(seasons = seasons) %>%
filter(season_type == 2,
team_display_name == team_name) %>%
transmute(
game_id,
game_date,
side   = team_home_away,
net    = team_score - opponent_team_score,
efg    = (field_goals_made + 0.5*three_point_field_goals_made) /
field_goals_attempted
) %>%
arrange(game_date) %>%
slice(1:6)
# 2) Pull in your pre-computed rolling stats
recent <- compute_recent_stats(seasons = seasons, window = window)
# 3) Merge together
check_df <- raw_lakers %>%
left_join(recent, by = "game_id") %>%
# pick the correct rolling column per side
mutate(
roll_net = if_else(side == "home", roll5_net_home, roll5_net_away),
roll_efg = if_else(side == "home", roll5_efg_home, roll5_efg_away)
) %>%
select(game_date, side, net, efg, roll_net, roll_efg)
print(check_df)
View(check_df)
model <- load_model
model <- load_model
train_feats <- prepare_train_test()$train_feats
train_feats <- prepare_train_test()$train_feats
devtools::load_all(".")
train_feats <- prepare_train_test()$train_feats
test_feats  <- prepare_train_test()$test_feats
team_name <- "Los Angeles Lakers"   # or whatever the unique() call showed
raw_lakers <- hoopR::load_nba_team_box(seasons = 2002) %>%
filter(season_type == 2,
team_display_name == team_name) %>%
transmute(
game_id,
game_date,
side = team_home_away,
net  = team_score - opponent_team_score,
efg  = (field_goals_made + 0.5 * three_point_field_goals_made) /
field_goals_attempted
) %>%
arrange(game_date) %>%
slice(1:6)
recent <- compute_recent_stats(seasons = 2002, window = 5)
check_df <- raw_lakers %>%
left_join(recent, by = "game_id") %>%
mutate(
roll_net = if_else(side=="home", roll5_net_home, roll5_net_away),
roll_efg = if_else(side=="home", roll5_efg_home, roll5_efg_away)
) %>%
select(game_date, side, net, efg, roll_net, roll_efg)
print(check_df)
devtools::load_all(".")
res <- prepare_train_test()
train_feats <- res$train_feats
test_feats  <- res$test_feats
names(train_feats)
head(train_feats[, c("net_diff5","efg_diff5")])
final_model <- glm(
Target ~ Elo_Diff + AllTimeElo_Diff + net_diff5 + efg_diff5,
data   = train_feats,
family = binomial()
)
View(train_feats)
present_model(final_model, train_feats, test_feats)
library(xgboost)
install.packages("xgboost")
library(xgboost)
library(Matrix)
# 1) Prepare sparse matrix
predictors <- c("Elo_Diff","AllTimeElo_Diff","net_diff5","efg_diff5")
dtrain <- xgb.DMatrix(
data  = as.matrix(train_feats[,predictors]),
label = train_feats$Target
)
dtest <- xgb.DMatrix(
data  = as.matrix(test_feats[,predictors]),
label = test_feats$Target
)
# 2) Train
params <- list(
objective = "binary:logistic",
eval_metric = "auc",
max_depth = 3,
eta = 0.1
)
bst <- xgb.train(
params = params,
data   = dtrain,
nrounds = 100,
watchlist = list(train = dtrain)
)
# 3) Predict & evaluate
probs_xgb <- predict(bst, dtest)
roc_xgb   <- pROC::roc(test_feats$Target, probs_xgb)
auc(roc_xgb)    # see your new AUC
library(caret)
library(pROC)
# 1) Accuracy at 0.5
preds_xgb_05 <- as.integer(probs_xgb > 0.5)
acc_05       <- mean(preds_xgb_05 == test_feats$Target)
cm_05        <- confusionMatrix(
factor(preds_xgb_05, levels = c(0,1)),
factor(test_feats$Target, levels = c(0,1))
)
cat(sprintf("XGBoost @ 0.5 → Accuracy: %.3f, Sensitivity: %.3f, Specificity: %.3f\n",
acc_05,
cm_05$byClass["Sensitivity"],
cm_05$byClass["Specificity"]))
# 2) Optimal threshold by Youden's J
roc_xgb    <- roc(test_feats$Target, probs_xgb)
opt_xgb    <- coords(roc_xgb, "best", best.method="youden", ret="threshold") %>% as.numeric()
preds_xgb_opt <- as.integer(probs_xgb > opt_xgb)
acc_opt       <- mean(preds_xgb_opt == test_feats$Target)
cm_opt        <- confusionMatrix(
factor(preds_xgb_opt, levels = c(0,1)),
factor(test_feats$Target, levels = c(0,1))
)
cat(sprintf("XGBoost @ %.3f → Accuracy: %.3f, Sensitivity: %.3f, Specificity: %.3f\n",
opt_xgb,
acc_opt,
cm_opt$byClass["Sensitivity"],
cm_opt$byClass["Specificity"]))
usethis::use_data(final_model, overwrite = TRUE)
model < load_model()
model < load_model()
devtools::load_all(".")
model < load_model()
model < load_model()
final_model < load_model()
model <- load_model()
train_feats <- prepare_train_test()$train_feats
test_feats  <- prepare_train_test()$test_feats
present_model(model, train_feats, test_feats)
